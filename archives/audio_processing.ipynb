{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137/segment_1.mp3\n",
      "Exported AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137/segment_2.mp3\n",
      "Exported AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137/segment_3.mp3\n",
      "Exported AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137/segment_4.mp3\n",
      "Exported AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137/segment_5.mp3\n",
      "Exported AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137/segment_6.mp3\n",
      "Exported AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137/segment_7.mp3\n",
      "Exported AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137/segment_8.mp3\n",
      "Exported AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137/segment_9.mp3\n",
      "Exported AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137/segment_10.mp3\n",
      "Exported AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137/segment_11.mp3\n",
      "Exported AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137/segment_12.mp3\n",
      "Exported AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137/segment_13.mp3\n",
      "Exported AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137/segment_14.mp3\n",
      "Exported AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137/segment_15.mp3\n",
      "Exported AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137/segment_16.mp3\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def split_mp3(filename, num_segments):\n",
    "    ''' Evenly split an mp3 file into num_segments segments '''\n",
    "    os.makedirs(filename[:-4], exist_ok=True)\n",
    "    audio = AudioSegment.from_mp3(filename)\n",
    "    duration_ms = len(audio)\n",
    "    segment_length = duration_ms // num_segments\n",
    "    for i in range(0, duration_ms, segment_length):\n",
    "        segment = audio[i:i + segment_length]\n",
    "        segment_filename = filename[:-4] + f\"/segment_{i // segment_length + 1}.mp3\"\n",
    "        segment.export(segment_filename, format=\"mp3\")\n",
    "        print(f\"Exported {segment_filename}\")\n",
    "\n",
    "path_to_file = 'AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137.mp3'\n",
    "\n",
    "num_segments = 15\n",
    "split_mp3(path_to_file, num_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported AudioData/AAPL/2020-Apr-30-AAPL.OQ-140195689057\n",
      "Exported AudioData/AAPL/2020-Jan-28-AAPL.OQ-137948852907\n",
      "Exported AudioData/AAPL/2019-Jan-29-AAPL.OQ-140336997647\n",
      "Exported AudioData/AAPL/2023-Aug-03-AAPL.OQ-138336763416\n",
      "Exported AudioData/AAPL/2021-Jan-27-AAPL.OQ-137405328510\n",
      "Exported AudioData/AAPL/2021-Apr-28-AAPL.OQ-139264952632\n",
      "Exported AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137\n",
      "Exported AudioData/AAPL/2020-Oct-29-AAPL.OQ-141103256170\n",
      "Exported AudioData/AAPL/2022-Oct-27-AAPL.OQ-140449384074\n",
      "Exported AudioData/AAPL/2022-Apr-28-AAPL.OQ-138491813375\n",
      "Exported AudioData/AAPL/2019-Oct-30-AAPL.OQ-137802390954\n",
      "Exported AudioData/AAPL/2023-Nov-02-AAPL.OQ-140502977515\n",
      "Exported AudioData/AAPL/2021-Jul-27-AAPL.OQ-138310703827\n",
      "Exported AudioData/AAPL/2021-Oct-28-AAPL.OQ-139435924054\n",
      "Exported AudioData/AAPL/2023-Feb-02-AAPL.OQ-140682524715\n",
      "Exported AudioData/AAPL/2022-Jan-27-AAPL.OQ-138984465849\n",
      "Exported AudioData/AAPL/2024-Feb-01-AAPL.OQ-139920838259\n",
      "Exported AudioData/AAPL/2019-Jul-30-AAPL.OQ-139263252221\n",
      "Exported AudioData/AAPL/2023-May-04-AAPL.OQ-139016265469\n",
      "Exported AudioData/AAPL/2022-Jul-28-AAPL.OQ-139999304673\n",
      "Exported AudioData/AAPL/2020-Jul-30-AAPL.OQ-139668219181\n",
      "Exported AudioData/GOOGL/2023-Apr-25-GOOGL.OQ-140631444702\n",
      "Exported AudioData/GOOGL/2019-Apr-29-GOOGL.OQ-138437793323\n",
      "Exported AudioData/GOOGL/2022-Apr-26-GOOGL.OQ-138254363287\n",
      "Exported AudioData/GOOGL/2020-Oct-29-GOOGL.OQ-139782001487\n",
      "Exported AudioData/GOOGL/2022-Oct-25-GOOGL.OQ-139231849236\n",
      "Exported AudioData/GOOGL/2020-Apr-28-GOOGL.OQ-140392485906\n",
      "Exported AudioData/GOOGL/2021-Oct-26-GOOGL.OQ-138977760639\n",
      "Exported AudioData/GOOGL/2019-Oct-28-GOOGL.OQ-140771052031\n",
      "Exported AudioData/GOOGL/2024-Jan-30-GOOGL.OQ-137240585407\n",
      "Exported AudioData/GOOGL/2020-Jul-30-GOOGL.OQ-137802974054\n",
      "Exported AudioData/GOOGL/2019-Jul-25-GOOGL.OQ-138968198385\n",
      "Exported AudioData/GOOGL/2023-Jul-25-GOOGL.OQ-137436048683\n",
      "Exported AudioData/GOOGL/2022-Jul-26-GOOGL.OQ-140228385298\n",
      "Exported AudioData/GOOGL/2023-Oct-24-GOOGL.OQ-139817940018\n",
      "Exported AudioData/GOOGL/2022-Feb-01-GOOGL.OQ-138419589371\n",
      "Exported AudioData/GOOGL/2021-Jul-27-GOOGL.OQ-139538083691\n",
      "Exported AudioData/GOOGL/2020-Feb-03-GOOGL.OQ-140620477566\n",
      "Exported AudioData/GOOGL/2023-Feb-02-GOOGL.OQ-139696880209\n",
      "Exported AudioData/GOOGL/2021-Apr-27-GOOGL.OQ-140883667426\n",
      "Exported AudioData/GOOGL/2021-Feb-02-GOOGL.OQ-140272639014\n",
      "Exported AudioData/MSFT/2021-Apr-27-MSFT.OQ-138354673931\n",
      "Exported AudioData/MSFT/2020-Jan-29-MSFT.OQ-140129439974\n",
      "Exported AudioData/MSFT/2023-Jan-24-MSFT.OQ-139975820358\n",
      "Exported AudioData/MSFT/2019-Jan-30-MSFT.OQ-138292870495\n",
      "Exported AudioData/MSFT/2023-Oct-24-MSFT.OQ-139993350133\n",
      "Exported AudioData/MSFT/2022-Oct-25-MSFT.OQ-137469915199\n",
      "Exported AudioData/MSFT/2019-Jul-18-MSFT.OQ-139110662748\n",
      "Exported AudioData/MSFT/2020-Apr-29-MSFT.OQ-140314894650\n",
      "Exported AudioData/MSFT/2021-Jan-26-MSFT.OQ-137814513205\n",
      "Exported AudioData/MSFT/2019-Apr-24-MSFT.OQ-137229165252\n",
      "Exported AudioData/MSFT/2023-Jul-25-MSFT.OQ-137462259693\n",
      "Exported AudioData/MSFT/2022-Jul-26-MSFT.OQ-139232561311\n",
      "Exported AudioData/MSFT/2020-Oct-27-MSFT.OQ-139733835758\n",
      "Exported AudioData/MSFT/2019-Oct-23-MSFT.OQ-138436529717\n",
      "Exported AudioData/MSFT/2021-Jul-27-MSFT.OQ-138684941105\n",
      "Exported AudioData/MSFT/2018-Oct-24-MSFT.OQ-140974450991\n",
      "Exported AudioData/MSFT/2022-Jan-25-MSFT.OQ-140806504057\n",
      "Exported AudioData/MSFT/2022-Apr-26-MSFT.OQ-138905362958\n",
      "Exported AudioData/MSFT/2023-Apr-25-MSFT.OQ-139717566700\n",
      "Exported AudioData/MSFT/2021-Oct-26-MSFT.OQ-137114240470\n",
      "Exported AudioData/MSFT/2024-Jan-30-MSFT.OQ-139756114312\n",
      "Exported AudioData/MSFT/2020-Jul-22-MSFT.OQ-137245175690\n",
      "Exported AudioData/NVDA/2020-Feb-13-NVDA.OQ-137146670911\n",
      "Exported AudioData/NVDA/2019-May-16-NVDA.OQ-140275786112\n",
      "Exported AudioData/NVDA/2023-Nov-21-NVDA.OQ-137385262309\n",
      "Exported AudioData/NVDA/2020-Nov-18-NVDA.OQ-137342634445\n",
      "Exported AudioData/NVDA/2020-Aug-19-NVDA.OQ-140529591121\n",
      "Exported AudioData/NVDA/2022-May-25-NVDA.OQ-140360204206\n",
      "Exported AudioData/NVDA/2021-Aug-18-NVDA.OQ-137355070866\n",
      "Exported AudioData/NVDA/2021-Feb-24-NVDA.OQ-140975780833\n",
      "Exported AudioData/NVDA/2021-May-26-NVDA.OQ-140536594409\n",
      "Exported AudioData/NVDA/2024-Feb-21-NVDA.OQ-137669438300\n",
      "Exported AudioData/NVDA/2023-Aug-23-NVDA.OQ-139752558276\n",
      "Exported AudioData/NVDA/2021-Nov-17-NVDA.OQ-139568780056\n",
      "Exported AudioData/NVDA/2018-May-10-NVDA.OQ-140924003184\n",
      "Exported AudioData/NVDA/2022-Feb-16-NVDA.OQ-140007769507\n",
      "Exported AudioData/NVDA/2018-Aug-16-NVDA.OQ-136946161445\n",
      "Exported AudioData/NVDA/2019-Aug-15-NVDA.OQ-140926561998\n",
      "Exported AudioData/NVDA/2020-May-21-NVDA.OQ-139958608589\n",
      "Exported AudioData/NVDA/2023-May-24-NVDA.OQ-136909784191\n",
      "Exported AudioData/NVDA/2018-Nov-15-NVDA.OQ-141007714603\n",
      "Exported AudioData/NVDA/2022-Nov-16-NVDA.OQ-139288301318\n",
      "Exported AudioData/NVDA/2023-Feb-22-NVDA.OQ-137608967498\n",
      "Exported AudioData/NVDA/2019-Nov-14-NVDA.OQ-138044908915\n",
      "Exported AudioData/NVDA/2019-Feb-14-NVDA.OQ-138662621679\n",
      "Exported AudioData/NVDA/2022-Aug-24-NVDA.OQ-138269873856\n"
     ]
    }
   ],
   "source": [
    "# go through all mp3 files in the directory and subdirectories and split them\n",
    "def split_all(path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp3'):\n",
    "                filename = file.split('/')[-1][:-4]\n",
    "                company = root.split('/')[-1]\n",
    "                output_dir = 'AudioData/' + company + '/' + filename\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                audio = AudioSegment.from_mp3(root + '/' + file)\n",
    "                duration_ms = len(audio)\n",
    "                segment_length = duration_ms // num_segments\n",
    "                for i in range(0, duration_ms, segment_length):\n",
    "                    segment = audio[i:i + segment_length]\n",
    "                    segment_filename = output_dir + f\"/segment_{i // segment_length + 1}.mp3\"\n",
    "                    segment.export(segment_filename, format=\"mp3\")\n",
    "                print(f\"Exported {output_dir}\")\n",
    "                # delete segment 16 if exists\n",
    "                if os.path.exists(output_dir + '/segment_16.mp3'):\n",
    "                    os.remove(output_dir + '/segment_16.mp3', )\n",
    "\n",
    "split_all('Audio/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxime/Desktop/Projects/Earning-Calls-Processor/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of EmotionModel were not initialized from the model checkpoint at audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def librosa_process(y, sr=16000):\n",
    "    tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    rmse = librosa.feature.rms(y=y)\n",
    "    spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    return tempo, chroma_stft.mean(), rmse.mean(), spec_cent.mean(), spec_bw.mean(), rolloff.mean(), zcr.mean(), mfcc.mean()\n",
    "\n",
    "\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2PreTrainedModel,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RegressionHead(nn.Module):\n",
    "    r\"\"\"Classification head.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EmotionModel(Wav2Vec2PreTrainedModel):\n",
    "    r\"\"\"Speech emotion classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = RegressionHead(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "    ):\n",
    "\n",
    "        outputs = self.wav2vec2(input_values)\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = torch.mean(hidden_states, dim=1)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        return hidden_states, logits\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = 'audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim'\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = EmotionModel.from_pretrained(model_name)\n",
    "\n",
    "def wave2vec_process(y):\n",
    "    y_processed = processor(y, sampling_rate=16000)\n",
    "    y_processed = y_processed['input_values'][0]\n",
    "    y_processed = y_processed.reshape(1, -1)\n",
    "    y_processed = torch.from_numpy(y_processed).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "            y_processed, sent = model(y_processed)\n",
    "\n",
    "    # convert to numpy\n",
    "    y_processed = y_processed.detach().cpu().numpy()\n",
    "    sent = sent.detach().cpu().numpy()\n",
    "\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ClapModel, ClapProcessor\n",
    "\n",
    "\n",
    "model_clap = ClapModel.from_pretrained(\"laion/larger_clap_general\")\n",
    "processor_clap = ClapProcessor.from_pretrained(\"laion/larger_clap_general\")\n",
    "\n",
    "def clap_process(y):\n",
    "    inputs = processor_clap(audios=y, return_tensors=\"pt\")\n",
    "    audio_embed = model_clap.get_audio_features(**inputs)\n",
    "    return audio_embed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract company and date from a format like 2020-Jul-30-AAPL.OQ-139668219181\n",
    "def extract_company_date(file):\n",
    "    date = file.split('-')[0] + '-' + file.split('-')[1] + '-' + file.split('-')[2]\n",
    "    company = file.split('-')[3]\n",
    "    return company, date\n",
    "\n",
    "\n",
    "def get_features(path, librosa_ = True, wave2vec = False, embeddings=False):\n",
    "    ''' Get features for each segment of the audio file'''\n",
    "    clap_features = {}\n",
    "    features = {}\n",
    "    for i in range(1, 16):\n",
    "        if librosa_:\n",
    "            features[f'company_{i}'] = []\n",
    "            features[f'date_{i}'] = []\n",
    "            features[f'tempo_segment_{i}'] = []\n",
    "            features[f'chroma_stft_segment_{i}'] = []\n",
    "            features[f'rmse_segment_{i}'] = []\n",
    "            features[f'spec_cent_segment_{i}'] = []\n",
    "            features[f'spec_bw_segment_{i}'] = []\n",
    "            features[f'rolloff_segment_{i}'] = []\n",
    "            features[f'zcr_segment_{i}'] = []\n",
    "            features[f'mfcc_segment_{i}'] = []\n",
    "\n",
    "        if wave2vec:\n",
    "            features[f'arousal_segment_{i}'] = []\n",
    "            features[f'valence_segment_{i}'] = []\n",
    "            features[f'dominance_segment_{i}'] = []\n",
    "        \n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp3'):\n",
    "                company, date = extract_company_date(root.split('/')[-1])\n",
    "                # extract date from root\n",
    "                y, _ = librosa.load(root + '/' + file, sr=16000)\n",
    "                if librosa_:\n",
    "                    tempo, chroma, rmse, spec_cent, spec_bw, rolloff, zcr, mfcc = librosa_process(y)\n",
    "                    segment = int(file.split('_')[-1][:-4])\n",
    "                    features[f'company_{segment}'].append(company)\n",
    "                    features[f'date_{segment}'].append(date)\n",
    "                    features[f'tempo_segment_{segment}'].append(tempo)\n",
    "                    features[f'chroma_stft_segment_{segment}'].append(chroma)\n",
    "                    features[f'rmse_segment_{segment}'].append(rmse)\n",
    "                    features[f'spec_cent_segment_{segment}'].append(spec_cent)\n",
    "                    features[f'spec_bw_segment_{segment}'].append(spec_bw)\n",
    "                    features[f'rolloff_segment_{segment}'].append(rolloff)\n",
    "                    features[f'zcr_segment_{segment}'].append(zcr)\n",
    "                    features[f'mfcc_segment_{segment}'].append(mfcc)\n",
    "                if wave2vec:\n",
    "                    arousal, valence, dominance = wave2vec_process(y)\n",
    "                    features[f'arousal_segment_{segment}'].append(arousal)\n",
    "                    features[f'valence_segment_{segment}'].append(valence)\n",
    "                    features[f'dominance_segment_{segment}'].append(dominance)\n",
    "                if embeddings:\n",
    "                    clap_features[(company, date, segment)] = clap_process(y)\n",
    "    return features, clap_features\n",
    "                        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, clap_features = get_features('AudioData/', librosa_=True, wave2vec=False, embeddings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company_1 87\n",
      "date_1 87\n",
      "tempo_segment_1 87\n",
      "chroma_stft_segment_1 87\n",
      "rmse_segment_1 87\n",
      "spec_cent_segment_1 87\n",
      "spec_bw_segment_1 87\n",
      "rolloff_segment_1 87\n",
      "zcr_segment_1 87\n",
      "mfcc_segment_1 87\n",
      "company_2 87\n",
      "date_2 87\n",
      "tempo_segment_2 87\n",
      "chroma_stft_segment_2 87\n",
      "rmse_segment_2 87\n",
      "spec_cent_segment_2 87\n",
      "spec_bw_segment_2 87\n",
      "rolloff_segment_2 87\n",
      "zcr_segment_2 87\n",
      "mfcc_segment_2 87\n",
      "company_3 87\n",
      "date_3 87\n",
      "tempo_segment_3 87\n",
      "chroma_stft_segment_3 87\n",
      "rmse_segment_3 87\n",
      "spec_cent_segment_3 87\n",
      "spec_bw_segment_3 87\n",
      "rolloff_segment_3 87\n",
      "zcr_segment_3 87\n",
      "mfcc_segment_3 87\n",
      "company_4 87\n",
      "date_4 87\n",
      "tempo_segment_4 87\n",
      "chroma_stft_segment_4 87\n",
      "rmse_segment_4 87\n",
      "spec_cent_segment_4 87\n",
      "spec_bw_segment_4 87\n",
      "rolloff_segment_4 87\n",
      "zcr_segment_4 87\n",
      "mfcc_segment_4 87\n",
      "company_5 87\n",
      "date_5 87\n",
      "tempo_segment_5 87\n",
      "chroma_stft_segment_5 87\n",
      "rmse_segment_5 87\n",
      "spec_cent_segment_5 87\n",
      "spec_bw_segment_5 87\n",
      "rolloff_segment_5 87\n",
      "zcr_segment_5 87\n",
      "mfcc_segment_5 87\n",
      "company_6 87\n",
      "date_6 87\n",
      "tempo_segment_6 87\n",
      "chroma_stft_segment_6 87\n",
      "rmse_segment_6 87\n",
      "spec_cent_segment_6 87\n",
      "spec_bw_segment_6 87\n",
      "rolloff_segment_6 87\n",
      "zcr_segment_6 87\n",
      "mfcc_segment_6 87\n",
      "company_7 87\n",
      "date_7 87\n",
      "tempo_segment_7 87\n",
      "chroma_stft_segment_7 87\n",
      "rmse_segment_7 87\n",
      "spec_cent_segment_7 87\n",
      "spec_bw_segment_7 87\n",
      "rolloff_segment_7 87\n",
      "zcr_segment_7 87\n",
      "mfcc_segment_7 87\n",
      "company_8 87\n",
      "date_8 87\n",
      "tempo_segment_8 87\n",
      "chroma_stft_segment_8 87\n",
      "rmse_segment_8 87\n",
      "spec_cent_segment_8 87\n",
      "spec_bw_segment_8 87\n",
      "rolloff_segment_8 87\n",
      "zcr_segment_8 87\n",
      "mfcc_segment_8 87\n",
      "company_9 87\n",
      "date_9 87\n",
      "tempo_segment_9 87\n",
      "chroma_stft_segment_9 87\n",
      "rmse_segment_9 87\n",
      "spec_cent_segment_9 87\n",
      "spec_bw_segment_9 87\n",
      "rolloff_segment_9 87\n",
      "zcr_segment_9 87\n",
      "mfcc_segment_9 87\n",
      "company_10 87\n",
      "date_10 87\n",
      "tempo_segment_10 87\n",
      "chroma_stft_segment_10 87\n",
      "rmse_segment_10 87\n",
      "spec_cent_segment_10 87\n",
      "spec_bw_segment_10 87\n",
      "rolloff_segment_10 87\n",
      "zcr_segment_10 87\n",
      "mfcc_segment_10 87\n",
      "company_11 87\n",
      "date_11 87\n",
      "tempo_segment_11 87\n",
      "chroma_stft_segment_11 87\n",
      "rmse_segment_11 87\n",
      "spec_cent_segment_11 87\n",
      "spec_bw_segment_11 87\n",
      "rolloff_segment_11 87\n",
      "zcr_segment_11 87\n",
      "mfcc_segment_11 87\n",
      "company_12 87\n",
      "date_12 87\n",
      "tempo_segment_12 87\n",
      "chroma_stft_segment_12 87\n",
      "rmse_segment_12 87\n",
      "spec_cent_segment_12 87\n",
      "spec_bw_segment_12 87\n",
      "rolloff_segment_12 87\n",
      "zcr_segment_12 87\n",
      "mfcc_segment_12 87\n",
      "company_13 87\n",
      "date_13 87\n",
      "tempo_segment_13 87\n",
      "chroma_stft_segment_13 87\n",
      "rmse_segment_13 87\n",
      "spec_cent_segment_13 87\n",
      "spec_bw_segment_13 87\n",
      "rolloff_segment_13 87\n",
      "zcr_segment_13 87\n",
      "mfcc_segment_13 87\n",
      "company_14 87\n",
      "date_14 87\n",
      "tempo_segment_14 87\n",
      "chroma_stft_segment_14 87\n",
      "rmse_segment_14 87\n",
      "spec_cent_segment_14 87\n",
      "spec_bw_segment_14 87\n",
      "rolloff_segment_14 87\n",
      "zcr_segment_14 87\n",
      "mfcc_segment_14 87\n",
      "company_15 87\n",
      "date_15 87\n",
      "tempo_segment_15 87\n",
      "chroma_stft_segment_15 87\n",
      "rmse_segment_15 87\n",
      "spec_cent_segment_15 87\n",
      "spec_bw_segment_15 87\n",
      "rolloff_segment_15 87\n",
      "zcr_segment_15 87\n",
      "mfcc_segment_15 87\n"
     ]
    }
   ],
   "source": [
    "for key in features:\n",
    "    print(key, len(features[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  company_1       date_1  tempo_segment_1  chroma_stft_segment_1  \\\n",
      "0   AAPL.OQ  2020-Jul-30       117.187500               0.365996   \n",
      "1   AAPL.OQ  2022-Jul-28       117.187500               0.364630   \n",
      "2   AAPL.OQ  2023-Feb-02        98.684211               0.352188   \n",
      "3   AAPL.OQ  2023-Nov-02       125.000000               0.351912   \n",
      "4   AAPL.OQ  2021-Jul-27       117.187500               0.371816   \n",
      "\n",
      "   rmse_segment_1  spec_cent_segment_1  spec_bw_segment_1  rolloff_segment_1  \\\n",
      "0        0.027201          1436.623670        1135.887954        2656.791836   \n",
      "1        0.016014          1093.471158        1067.038987        2113.469127   \n",
      "2        0.042191          1130.034723        1079.233129        2186.137105   \n",
      "3        0.027063          1052.445032        1035.682371        2025.102747   \n",
      "4        0.039081          1368.223525        1144.997330        2593.960984   \n",
      "\n",
      "   zcr_segment_1  mfcc_segment_1  ... company_15      date_15  \\\n",
      "0       0.138336      -20.716206  ...    AAPL.OQ  2020-Jul-30   \n",
      "1       0.093369      -17.780802  ...    AAPL.OQ  2022-Jul-28   \n",
      "2       0.097409      -13.370982  ...    AAPL.OQ  2023-Feb-02   \n",
      "3       0.085882      -15.141086  ...    AAPL.OQ  2023-Nov-02   \n",
      "4       0.121676      -17.858797  ...    AAPL.OQ  2021-Jul-27   \n",
      "\n",
      "   tempo_segment_15  chroma_stft_segment_15  rmse_segment_15  \\\n",
      "0        125.000000                0.385950         0.027179   \n",
      "1        104.166667                0.378048         0.014983   \n",
      "2        133.928571                0.378355         0.026675   \n",
      "3        110.294118                0.360515         0.027993   \n",
      "4        104.166667                0.374521         0.047144   \n",
      "\n",
      "   spec_cent_segment_15  spec_bw_segment_15  rolloff_segment_15  \\\n",
      "0           1319.318831         1067.288287         2400.524408   \n",
      "1           1435.973208         1179.260750         2689.323728   \n",
      "2           1434.207016         1175.716263         2679.458920   \n",
      "3           1324.005457         1119.444386         2475.556736   \n",
      "4           1399.364625         1128.273892         2573.490449   \n",
      "\n",
      "   zcr_segment_15  mfcc_segment_15  \n",
      "0        0.128421       -19.650646  \n",
      "1        0.130930       -23.756828  \n",
      "2        0.133043       -21.197886  \n",
      "3        0.117462       -18.598307  \n",
      "4        0.132546       -17.081926  \n",
      "\n",
      "[5 rows x 150 columns]\n"
     ]
    }
   ],
   "source": [
    "# dic --> dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# create dataframe from dictionary of lists\n",
    "df = pd.DataFrame(features)\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv('AudioData/features_audio.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clap features as json file\n",
    "import json\n",
    "\n",
    "with open('AudioData/clap_features.json', 'w') as f:\n",
    "    json.dump(clap_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio/ ['AAPL', 'GOOGL', 'MSFT', 'NVDA'] []\n",
      "Audio/AAPL [] ['2020-Apr-30-AAPL.OQ-140195689057.mp3', '2020-Jan-28-AAPL.OQ-137948852907.mp3', '2019-Jan-29-AAPL.OQ-140336997647.mp3', '2023-Aug-03-AAPL.OQ-138336763416.mp3', '2021-Jan-27-AAPL.OQ-137405328510.mp3', '2021-Apr-28-AAPL.OQ-139264952632.mp3', '2019-Apr-30-AAPL.OQ-139934481137.mp3', '2020-Oct-29-AAPL.OQ-141103256170.mp3', '2022-Oct-27-AAPL.OQ-140449384074.mp3', '2022-Apr-28-AAPL.OQ-138491813375.mp3', '2019-Oct-30-AAPL.OQ-137802390954.mp3', '2023-Nov-02-AAPL.OQ-140502977515.mp3', '2021-Jul-27-AAPL.OQ-138310703827.mp3', '2021-Oct-28-AAPL.OQ-139435924054.mp3', '2023-Feb-02-AAPL.OQ-140682524715.mp3', '2022-Jan-27-AAPL.OQ-138984465849.mp3', '2024-Feb-01-AAPL.OQ-139920838259.mp3', '2019-Jul-30-AAPL.OQ-139263252221.mp3', '2023-May-04-AAPL.OQ-139016265469.mp3', '2022-Jul-28-AAPL.OQ-139999304673.mp3', '2020-Jul-30-AAPL.OQ-139668219181.mp3']\n",
      "Audio/GOOGL [] ['2023-Apr-25-GOOGL.OQ-140631444702.mp3', '2019-Apr-29-GOOGL.OQ-138437793323.mp3', '2022-Apr-26-GOOGL.OQ-138254363287.mp3', '2020-Oct-29-GOOGL.OQ-139782001487.mp3', '2022-Oct-25-GOOGL.OQ-139231849236.mp3', '2020-Apr-28-GOOGL.OQ-140392485906.mp3', '2021-Oct-26-GOOGL.OQ-138977760639.mp3', '2019-Oct-28-GOOGL.OQ-140771052031.mp3', '2024-Jan-30-GOOGL.OQ-137240585407.mp3', '2020-Jul-30-GOOGL.OQ-137802974054.mp3', '2019-Jul-25-GOOGL.OQ-138968198385.mp3', '2023-Jul-25-GOOGL.OQ-137436048683.mp3', '2022-Jul-26-GOOGL.OQ-140228385298.mp3', '2023-Oct-24-GOOGL.OQ-139817940018.mp3', '2022-Feb-01-GOOGL.OQ-138419589371.mp3', '2021-Jul-27-GOOGL.OQ-139538083691.mp3', '2020-Feb-03-GOOGL.OQ-140620477566.mp3', '2023-Feb-02-GOOGL.OQ-139696880209.mp3', '2021-Apr-27-GOOGL.OQ-140883667426.mp3', '2021-Feb-02-GOOGL.OQ-140272639014.mp3']\n",
      "Audio/MSFT [] ['2021-Apr-27-MSFT.OQ-138354673931.mp3', '2020-Jan-29-MSFT.OQ-140129439974.mp3', '2023-Jan-24-MSFT.OQ-139975820358.mp3', '2019-Jan-30-MSFT.OQ-138292870495.mp3', '2023-Oct-24-MSFT.OQ-139993350133.mp3', '2022-Oct-25-MSFT.OQ-137469915199.mp3', '2019-Jul-18-MSFT.OQ-139110662748.mp3', '2020-Apr-29-MSFT.OQ-140314894650.mp3', '2021-Jan-26-MSFT.OQ-137814513205.mp3', '2019-Apr-24-MSFT.OQ-137229165252.mp3', '2023-Jul-25-MSFT.OQ-137462259693.mp3', '2022-Jul-26-MSFT.OQ-139232561311.mp3', '2020-Oct-27-MSFT.OQ-139733835758.mp3', '2019-Oct-23-MSFT.OQ-138436529717.mp3', '2021-Jul-27-MSFT.OQ-138684941105.mp3', '2018-Oct-24-MSFT.OQ-140974450991.mp3', '2022-Jan-25-MSFT.OQ-140806504057.mp3', '2022-Apr-26-MSFT.OQ-138905362958.mp3', '2023-Apr-25-MSFT.OQ-139717566700.mp3', '2021-Oct-26-MSFT.OQ-137114240470.mp3', '2024-Jan-30-MSFT.OQ-139756114312.mp3', '2020-Jul-22-MSFT.OQ-137245175690.mp3']\n",
      "Audio/NVDA [] ['2020-Feb-13-NVDA.OQ-137146670911.mp3', '2019-May-16-NVDA.OQ-140275786112.mp3', '2023-Nov-21-NVDA.OQ-137385262309.mp3', '2020-Nov-18-NVDA.OQ-137342634445.mp3', '2020-Aug-19-NVDA.OQ-140529591121.mp3', '2022-May-25-NVDA.OQ-140360204206.mp3', '2021-Aug-18-NVDA.OQ-137355070866.mp3', '2021-Feb-24-NVDA.OQ-140975780833.mp3', '2021-May-26-NVDA.OQ-140536594409.mp3', '2024-Feb-21-NVDA.OQ-137669438300.mp3', '2023-Aug-23-NVDA.OQ-139752558276.mp3', '2021-Nov-17-NVDA.OQ-139568780056.mp3', '2018-May-10-NVDA.OQ-140924003184.mp3', '2022-Feb-16-NVDA.OQ-140007769507.mp3', '2018-Aug-16-NVDA.OQ-136946161445.mp3', '2019-Aug-15-NVDA.OQ-140926561998.mp3', '2020-May-21-NVDA.OQ-139958608589.mp3', '2023-May-24-NVDA.OQ-136909784191.mp3', '2018-Nov-15-NVDA.OQ-141007714603.mp3', '2022-Nov-16-NVDA.OQ-139288301318.mp3', '2023-Feb-22-NVDA.OQ-137608967498.mp3', '2019-Nov-14-NVDA.OQ-138044908915.mp3', '2019-Feb-14-NVDA.OQ-138662621679.mp3', '2022-Aug-24-NVDA.OQ-138269873856.mp3']\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk('Audio/'):\n",
    "    print(root, dirs, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio file\n",
    "filename = 'AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'AudioData/AAPL/2019-Apr-30-AAPL.OQ-139934481137/segment_1.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(filename, sr=16000, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.2667331e-06,  7.3400183e-06,  8.1522548e-06, ...,\n",
       "        2.5624418e-04,  2.8384631e-04, -1.6588336e-04], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def librosa_process(y, sr=16000):\n",
    "    tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    rmse = librosa.feature.rms(y=y)\n",
    "    spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    # return single value for each\n",
    "    return tempo, chroma_stft.mean(), rmse.mean(), spec_cent.mean(), spec_bw.mean(), rolloff.mean(), zcr.mean(), mfcc.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144.23076923076923,\n",
       " 0.37024707,\n",
       " 0.009967313,\n",
       " 1594.1986496905806,\n",
       " 1222.5049879092867,\n",
       " 2933.5234284262488,\n",
       " 0.1484131851816565,\n",
       " -27.36655)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librosa_process(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EmotionModel were not initialized from the model checkpoint at audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_values': [array([ 0.00112327,  0.00156151,  0.00162012, ...,  0.0195219 ,\n",
       "        0.02151361, -0.01093792], dtype=float32)], 'attention_mask': [array([1, 1, 1, ..., 1, 1, 1], dtype=int32)]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "model_name = 'audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim'\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = EmotionModel.from_pretrained(model_name)\n",
    "processor(y, sampling_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.7991957068443298, 'label': 'positive sentiment'}, {'score': 0.08826709538698196, 'label': 'aggressive tone'}, {'score': 0.07042617350816727, 'label': 'calm tone'}, {'score': 0.02783028595149517, 'label': 'negative sentiment'}, {'score': 0.010820894502103329, 'label': 'neutral sentiment'}, {'score': 0.0018461584113538265, 'label': 'slow speech'}, {'score': 0.001613809261471033, 'label': 'fast speech'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "audio = y  # Your audio input\n",
    "\n",
    "# Initialize the zero-shot audio classification pipeline with a suitable model\n",
    "audio_classifier = pipeline(task=\"zero-shot-audio-classification\", model=\"laion/larger_clap_general\")\n",
    "\n",
    "# Define candidate labels that represent the audio features you are interested in\n",
    "candidate_labels = [\n",
    "    \"positive sentiment\",  # Looking for positive emotional content\n",
    "    \"negative sentiment\",  # Looking for negative emotional content\n",
    "    \"neutral sentiment\",   # Checking for neutral emotional content\n",
    "    \"fast speech\",         # Identifying fast speech rates\n",
    "    \"slow speech\",         # Identifying slow speech rates\n",
    "    \"calm tone\",           # Identifying calm tones\n",
    "    \"aggressive tone\"      # Identifying aggressive tones\n",
    "]\n",
    "\n",
    "# Classify the audio with the newly defined candidate labels\n",
    "output = audio_classifier(audio, candidate_labels=candidate_labels)\n",
    "\n",
    "# Print the classification output\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/Rajaram1996/Hubert_emotion/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
